---
title: 'Open Tests: Harvard Measurement Lab'
author: "Emma Dwight"
date: "7/29/2020"
output:
  html_document:
    df_print: paged
  md_document:
    variant: markdown_github
  pdf_document: default
  word_document: default
---


In this tutorial we show how schools, districts, and states can create and score a test comprised of publicly available questions. As our example, we use released items from the 2013 administration of NAEP 4th Grade Math.

# Ingredients:
1) Released test questions (items) to create the test, like these:

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/NationsReportCard/nqt/Search", out.width = '70%', fig.align="center"}
knitr::opts_chunk$set(fig.align="center", cache = TRUE)
knitr::include_graphics("images/1_released_items.png")
```

2) Released item IRT parameter estimates, like those found here: 
(Note that for NAEP, you will need to combine IRT parameters from each of the subscales to create an overall item pool)

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_irt_math.aspx", out.width = '70%', fig.align="center"}
knitr::include_graphics("images/2_item_parameters.png")
```

3) Common item numbers/identifiers that relate each question (item) to its parameters:
(To get the IDs for questions from NAEP, using the NAEP Questions Tool to select multiple choice questions from your chosen year, click OK, then select all items, then "Export to Excel") 

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/NationsReportCard/nqt/Search#", out.width = '70%', fig.align="center"}
knitr::include_graphics("images/3_questions_with_IDs.png")
```

4) Student scores. For our tutorial, these can either be sum scores, or correct/incorrect responses for each of the test questions. 
(For NAEP items, you should consider exploring "Download...Test Kit" for paper administration or "Publish Test Online" for online administration once you've chosen your items in the NAEP Questions Tool.)

5) A table or equation that converts theta scores to reporting scale scores (and possibly also achievement levels), like this:

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/nationsreportcard/tdw/analysis/2013/trans_constants_math2013.aspx", out.width = '70%', fig.align="center"}
knitr::include_graphics("images/5_theta_to_scale.png")
```

For NAEP, cut scores separating achievement level can be found here: https://nces.ed.gov/nationsreportcard/mathematics/achieve.asp#grade4

# Contents:
### Estimated Student Scale Scores from Sum Scores:
* Import 3PL IRT item parameters
* Import theta to scale score equation or table
* Import (or simulate) student response data
* Estimate student thetas using sum scores
* Convert estimated thetas to scale scores
* Produce table converting sum scores to scale scores, using Test Characteristic Curve information
* Export student ability data, including estimated thetas, scale scores, and achievement levels

### Appendix 1: Simulating Imprecision for Sum Score Theta Estimates
* Invert the Test Characteristic Curve to Produce Estimates of Standard Errors
* Produce table converting sum scores to scale scores, with empirical standard errors, using simulations

### Appendix 2: Scale Score Estimation Using Full-Pattern Scoring
* Estimate student ability using full-pattern scoring, (includes standard errors)
* Export student ability data, including estimated thetas, standard errors, scale scores, and achievement levels

### Appendix 3: Diagnostics
* Report Classical Test Theory statistics
* Plot Item Characteristic Curves and Test Characteristic Curve
* Plot Item Information Functions and Test Information Function


```{r, include = FALSE, message = FALSE}
# Install/update needed packages
#install.packages("dplyr")
#install.packages("irtoys")      # May require some dependencies

library(dplyr)
library(irtoys)
```

# Estimated Student Scale Scores from Sum Scores:

### Importing item parameters and scale scores

```{r}
# Import selected items
selected_items <- read.csv("Selected_Items.csv", skip = 1)

# Pull out vector of NAEP IDs for selected items
item_IDs <- selected_items$NAEP.ID

# Import item parameter data
item_parameters_raw <- read.csv("IRT_Parameters.csv")                            

# Subset the item parameters, keeping only those whose IDs are in item_IDs
item_parameters_selected <- subset(item_parameters_raw, item_parameters_raw$NAEP.ID %in% item_IDs)

# Drop some unnecessary rows
item_parameters <- item_parameters_selected %>% dplyr::select(NAEP.ID, aj, bj, cj, Subscale)

# Turn this into the matrix form that irtoys expects
my_ip <- as.matrix(item_parameters  %>% dplyr::select(aj, bj, cj))               

# NAEP Technical Documentation shows that a normalizing constant D of 1.7 is used in their 3PL equations:
# See here: https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_models_3pl.aspx
# The R package irtoys uses a normalizing constant of 1, so we adjust all NAEP parameters accordingly
D <- 1.7          # Change to D = 1 if none appears in your technical report
# Divide "a" parameters by D to "undo" the normalizing constant and fit what the package irtoys expects
my_ip[,1] <- my_ip[,1]/D

# View first five rows
my_ip[1:5, ]

# Import student responses (if using real data)


# Import theta -> scale score equations
scale_scores_raw <- read.csv("Scale_Score_Equations.csv")

# These scale score equations for NAEP are separated by subscale. We offer two possible methods for combining these for an overall math equation. Select whichever makes the most sense for your situation. 

# Takes theta(s), returns scale score(s)
# theta_to_scale_score <- function(theta)
# {
#   # Simplified version: just use the number properties and operations parameters
#   return(scale_score = scale_scores_raw$A[1]*theta + scale_scores_raw$B[1])
# }

# Takes theta(s), returns scale score(s)
theta_to_scale_score <- function(theta, weights = c(0.4, 0.2, 0.15, 0.1, 0.15))
{
  # Complex version: weighted average of each of the five subscale equations
  # Default weights taken from here for 2013 grade 4 Math: 
  # https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_determination_composite.aspx
  # Alternatively, user can provide weights customized to the actual distribution of questions used
  A = weighted.mean(x = scale_scores_raw$A, w = weights)
  B = weighted.mean(x = scale_scores_raw$B, w = weights)
  return(scale_score = A*theta + B)
}

# If applicable, this function takes scale scores and returns achievement levels

# Takes theta(s), returns scale score(s)
scale_score_to_achievement_level <- function(scale_scores, cutoffs = c(214, 249, 282))
{
  # Note that these cutscores can vary by grade, subject, and year
  # Default cutoffs taken from here for 2013 grade 4 Math: 
  # https://nces.ed.gov/nationsreportcard/mathematics/achieve.asp#grade4
  achievement_levels <- rep("Below Basic", length(scale_scores)) 
  achievement_levels[scale_scores >= cutoffs[1]] <- "Basic"
  achievement_levels[scale_scores >= cutoffs[2]] <- "Proficient"
  achievement_levels[scale_scores >= cutoffs[3]] <- "Advanced"
  return(achievement_levels)
}
```

### Scale Score Estimation Using Sum Scoring
"Invert" the TCC to get a map from sum scores to thetas:

```{r}
# Plot Test Characteristic Curve (which underlies all of what follows)
# Theta on x axis, predicted sum score on y axis
plot(trf(my_ip))

# Create a range of possible thetas
thetas = seq(from = -5, to = 5, length.out = 100001)
# Calculate the estimated sum score for each of these thetas 
all_sum_scores <- trf(my_ip, x = thetas)

# Simplifies the above possible sum scores (decimals) into possible whole number scores
possible_sum_scores <- seq(from = ceiling(all_sum_scores$f[1]), to = floor(all_sum_scores$f[100000]), by = 1)

# For each possible whole number score, find the associated theta that's the best match
matching_theta_scores <- rep(NA, times = length(possible_sum_scores))
for(i in 1:length(possible_sum_scores)) {matching_theta_scores[i] <- all_sum_scores$x[which.min(abs(all_sum_scores$f - possible_sum_scores[i]))]}

# Produce a table with sum scores, thetas, scale scores, and achievement levels
matching_theta_scores_df <- as.data.frame(cbind(possible_sum_scores, matching_theta_scores))
colnames(matching_theta_scores_df) <- c("SumScore", "Theta")

# Convert the thetas to scale scores and then achievement levels, using our custom functions above
matching_theta_scores_df$ScaleScore <- theta_to_scale_score(matching_theta_scores_df$Theta)
matching_theta_scores_df$AchievementLevel <- scale_score_to_achievement_level(matching_theta_scores_df$ScaleScore)

# Round the Scale Scores before reporting
matching_theta_scores_df$ScaleScore <- round(matching_theta_scores_df$ScaleScore)

# View table
matching_theta_scores_df

# Export table
write.csv(matching_theta_scores_df, file = "Estimated_Scale_Scores_From_Sum_Scores.csv", row.names = F)
```

# Appendix 1: Simulating Imprecision for Sum Score Theta Estimates

How do we convey a sense of uncertainty with these sorts of score estimates?

Because students with "fixed" thetas can retake the test and score slightly differently each time, we simulate this below to create an empirical range of possible scale scores for each student sum score.

```{r warning=FALSE, message = FALSE}
# Create a sequence of possible thetas, equally spaced
sim_thetas <- seq(from = -5, to = 5, length.out = 1001)

# Replicate each of the possible thetas 100 times, as if each of these students took the test 100 times
sim_thetas_n <- rep(sim_thetas, each = 100)

# Simulate the sum scores that these students received, using the item parameters for this test (assume these item parameters are estimated perfectly)
sim_sum_scores <- rowSums(sim(my_ip, sim_thetas_n))

# Combine the thetas and sum scores into a data frame
sim1_long <- as.data.frame(cbind(sim_thetas_n, sim_sum_scores))

# Rename column to the name expected by theta_to_scale_score conversion
colnames(sim1_long) <- c("Theta", "sim_sum_scores")

# Add column for scale scores (from thetas)
sim1_long$sim_scale_scores <- theta_to_scale_score(sim1_long$Theta)

# Group the above data frame by sum scores, and summarize the different "true" thetas that could have produced each sum score
sim1_thetas <- sim1_long %>% 
  group_by(sim_sum_scores) %>% 
  summarise(tibble('2.5' = round(quantile(sim_scale_scores, 0.025), 0),
                   '10' = round(quantile(sim_scale_scores, 0.1), 0),
                   mean = round(mean(sim_scale_scores), 0), 
                   median = round(median(sim_scale_scores), 0), 
                   se = round(sd(sim_scale_scores), 1),
                   '90' = round(quantile(sim_scale_scores, 0.9), 0),
                   '97.5' = round(quantile(sim_scale_scores, 0.975), 0), 
                   max = round(max(sim_scale_scores), 0),
                   nsims = n()))
sim1_thetas

# Repeat the above, this time summarizing the different "true" scale scores that could have produced each sum score
sim1_scale_scores <- sim1_long %>% 
  group_by(sim_sum_scores) %>% 
  summarise(tibble('2.5' = round(quantile(sim_scale_scores, 0.025), 0),
                   '10' = round(quantile(sim_scale_scores, 0.1), 0),
                   mean = round(mean(sim_scale_scores), 0), 
                   median = round(median(sim_scale_scores), 0), 
                   se = round(sd(sim_scale_scores), 1),
                   '90' = round(quantile(sim_scale_scores, 0.9), 0),
                   '97.5' = round(quantile(sim_scale_scores, 0.975), 0), 
                   max = round(max(sim_scale_scores), 0),
                   nsims = n()))
sim1_scale_scores

# Can also create a histogram for the possible thetas that produced each sum score, for example:
hist(sim1_long$Theta[sim1_long$sim_sum_scores == 20], freq = F)
```

# Appendix 2: Scale Score Estimation Using Full-Pattern Scoring

Full-pattern scoring (using a full table recording how each student performed on each question) can improve ability estimation.

```{r}
# Use parameter estimates to simulate answers for 100 students
# (Import student data here if using real answers)
set.seed(88)
sim_thetas <- rnorm(1000)
sim_responses <- sim(my_ip, sim_thetas)

# Put the parameter estimates and standard errors into the list structure that irtoys functions expect
# Note: ability estimation function does not need standard errors or variance-covariance matrix to run
parameter_list <- list(est = my_ip, se = NA, vcm = NA)

# Estimate student thetas, based on full-pattern scoring, using MLE (several other methods exist for this "ability" function)
mod_MLE<- ability(resp = sim_responses, ip = parameter_list, method = "MLE")

# Look at the first five rows
mod_MLE[1:5, ]

# Convert these estimated thetas to scale scores and achievement levels
ability_df <- as.data.frame(mod_MLE)
colnames(ability_df) <- c("Theta", "se(Theta)", "QuestionsAnswered")
ability_df$ScaleScore <- theta_to_scale_score(ability_df$Theta)
ability_df$AchievementLevel <- scale_score_to_achievement_level(ability_df$ScaleScore)

# Round the scale scores to 0 dp before reporting
ability_df$ScaleScore <- round(ability_df$ScaleScore, 0)

# View table:
ability_df

# Export the data 
write.csv(ability_df, file = "Estimated_Scores_From_Full_Pattern_Scores.csv", row.names = F)
```


# Appendix 3: Diagnostics

## Diagnostics and other test plots:
```{r}
# Simulate data
set.seed(88)
sim_thetas <- rnorm(1000)
sim_responses <- sim(my_ip, sim_thetas)

# Classical Test Theory EDA metrics:
# Note: Because the simulated data is pre-graded, we're saying the "answer key" is 1, 1, 1, 1, 1....
ctt <- tia(sim_responses, key = rep(1, 31))

# Show Cronbach's alpha for this "test":
ctt$testlevel$alpha

# Show CTT item-level statistics for first five items
ctt$itemlevel[1:5, ]

# Estimate student thetas, based on full-pattern scoring, using MLE (several other methods exist for this "ability" function)
mod_MLE<- ability(resp = sim_responses, ip = parameter_list, method = "MLE")

# How does the MLE ability estimation perform compared to the "true thetas" from our simulation?
cor(mod_MLE[,1], sim_thetas)
plot(sim_thetas, mod_MLE[,1])

# How do other ability estimation methods perform?
mod_WLE <- ability(resp = sim_responses, ip = parameter_list, method = "WLE")
cor(mod_WLE[,1], sim_thetas)
plot(sim_thetas, mod_WLE[,1])

# Item Characteristic Curves for first five items (this package calls them "item response functions")
plot(irf(my_ip, items = 1:5), co = NA, label = T)

# ICCs for all items:
plot(irf(my_ip), co = NA, label = T)

# Test Characteristic Curve (this package calls this a "test response function")
plot(trf(my_ip))

# Overlaid item information curves
plot(iif(my_ip))

# Test information function
plot(tif(my_ip))

# Cool plot of observed sum scores and predicted sum scores against estimated ability, with +/- 1se bands
scp(sim_responses, my_ip)

# "Empirical response function" for a selected item: observed sum scores vs. percent correct on this question
item_1_erf <- erf(sim_responses, 1)
```