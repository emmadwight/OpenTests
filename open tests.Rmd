---
title: "Open Tests: Harvard Measurement Lab"
author: "Emma Dwight"
date: "6/23/2020"
output: pdf_document
---
In this tutorial we create and score a test for MCAS 4th grade ELA, using simulated student responses, and released questions from 2018.

# Ingredients:
1) Released test questions (items) to create the test, like these: https://mcas.digitalitemlibrary.com/home?subject=ELA&grades=Grade%204&view=ALL

2) Released item IRT parameters, like table M5 here: http://www.mcasservicecenter.com/documents/MA/Technical%20Report/2018/NextGen/Appendix%20M%20-%20Plots%20and%20IRT%20Parameters.pdf 
Example file saved as "tablem5.xlsx"

3) Common item numbers/identifiers that relate each question (item) to its parameters: currently missing for MCAS

4) Student responses for each of the test questions, graded as correct/incorrect: we simulate fake data below
(Note: we plan to add code that could estimate student ability using only sum-scores, rather than full-pattern scores)

5) A table that converts theta scores to scale scores (and possibly also achievement levels), like table N2, here: http://www.mcasservicecenter.com/documents/MA/Technical%20Report/2018/NextGen/Appendix%20N%20-%20Scaled%20Score%20Distributions%20and%20Look-up%20Tables_4.17.19.pdf 
Example file saved as "tablen2.xlsx"

# Progress:
Implemented so far:
- Import 3PL item parameters
- Import theta to scale score table
- Simulate student responses, as full-pattern 0/1 scores 
- Estimate student ability on theta scale
- Convert theta scores to scale scores
- Estimate achievement levels for cutoffs known/given for scale scores
- Export student ability data, including thetas, scale scores, and achievement levels
- Report Classical Test Theory statistics
- Plot Item Characteristic Curves and Test Characteristic Curve
- Plot Item Information Functions and Test Information Function

To do next:
- Report standard errors on scale scores if possible?

Cool, harder to-dos:
- How should I think about standard errors on theta scale from ability estimation and the scale-score standard errors MCAS released? How should I report and explain standard errors?
- Implement sum scores -> scale scores methodology (estimation, probably sans standard errors)
- Add support for polytomously scored items? (Hand-scored, GRM stuff for MCAS)

Future extras/usability extensions:
- Extend functionality for 1PL and 2PL parameters, if that's how a different state does things (possibly just zero out the c column)
- Add code to import student responses, including student names/identifiers to attach to the thetas, scale scores, and 
- Add code to dichotomously score multi-choice questions if a key is provided (functions already exist for this, eg. irtoys::sco)

```{r, include = FALSE, message = FALSE}
# Install/update needed packages
install.packages("irtoys")      # May require some dependencies
install.packages("readxl")      # Only if importing .xls or .xlsx files, not needed if using .csv files
install.packages("dplyr")
library(dplyr)
```

```{r}
library(readxl)

# Import item parameter data
item_parameters_raw <- read_excel("tablem5.xlsx")                            
# Separate the item parameters from the standard errors
my_ip <- as.matrix(item_parameters_raw  %>% dplyr::select(a, b, c))               
my_se <- as.matrix(item_parameters_raw  %>% dplyr::select("se(a)", "se(b)", "se(c)"))

# Import student responses
# Note: use the "upload" function next to the file menu in the window on the right hand side of the console

# Import theta -> scale score table
scale_scores_raw <- read_excel("tablen2.xlsx")
# Pull out the relevant columns:
scale_scores <- scale_scores_raw  %>% dplyr::select("Theta", "Scale Score (2018)")

# Plot relationship between thetas and scale scores:
plot(x = scale_scores$Theta, y = scale_scores$`Scale Score (2018)`)

# Relationship is a straight line! Learn the model to be able to convert later.
theta_to_scale_score <- lm(`Scale Score (2018)` ~ Theta, data = scale_scores)
summary(theta_to_scale_score)
```

```{r}
# Load IRT library
library(irtoys)

# Use parameter estimates to simulate answers for 100 students
set.seed(88)
sim_thetas <- rnorm(100)
sim_responses <- sim(my_ip, sim_thetas)

# Put the parameter estimates and standard errors into the list structure that irtoys functions expect
# Note: ability estimation function does not need standard errors to run
parameter_list <- list(est = my_ip, se = my_se, vcm = NA)

# Estimate student thetas, based on full-pattern scoring, using MLE (several other methods exist in this "ability" function)
mod_MLE<- ability(resp = sim_responses, ip = parameter_list, method = "MLE")
mod_MLE

# Convert these estimated thetas to scale scores and achievement levels
ability_df <- as.data.frame(mod_MLE)
colnames(ability_df) <- c("Theta", "se(Theta)", "n")
ability_df$ScaleScore <- predict(theta_to_scale_score, newdata = ability_df)

ability_df$AchievementLevel <- 1
ability_df$AchievementLevel[ability_df$ScaleScore > 470] <- 2
ability_df$AchievementLevel[ability_df$ScaleScore > 500] <- 3
ability_df$AchievementLevel[ability_df$ScaleScore > 530] <- 4

# Look at the data
ability_df

# Round the scale scores to 0 dp before reporting
ability_df$ScaleScore <- round(ability_df$ScaleScore, 0)

# Export the data 
write.csv(ability_df, file = "EstimatedScores.csv", row.names = F)
# The above file will appear in the file window on the bottom-right hand side of the screen
# To download to local computer, select this file using the checkbox, then use More...Export...
```

Diagnostics:
```{r}
# Classical Test Theory EDA metrics:
# Note: Because the simulated data is pre-graded, we're saying the "answer key" is 1, 1, 1, 1, 1....
ctt <- tia(sim_responses, key = rep(1, 15))
ctt$testlevel$alpha
ctt$itemlevel

# How does the MLE ability estimation perform compared to the "true thetas" from our simulation?
cor(mod_MLE[,1], sim_thetas)
plot(sim_thetas, mod_MLE[,1])

# How do other ability estimation methods perform?
mod_WLE <- ability(resp = sim_responses, ip = parameter_list, method = "WLE")
cor(mod_WLE[,1], sim_thetas)
plot(sim_thetas, mod_WLE[,1])

# Item Characteristic Curves (this package calls them "item response functions")
plot(irf(my_ip), co = NA, label = T)
plot(irf(my_ip, items = 1:5), co = NA, label = T)

# Test Characteristic Curve (this package calls this a "test response function")
plot(trf(my_ip))

# Overlaid item information curves
plot(iif(my_ip))

# Test information function
plot(tif(my_ip))

# Cool plot of observed sum scores and predicted sum scores against estimated ability, with +/- 1se bands
scp(sim_responses, my_ip)

# "Empirical response function" for a selected item: observed sum scores vs. percent correct on this question
erf(sim_responses, 1)
```
