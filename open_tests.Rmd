---
title: 'Open Tests: Harvard Measurement Lab'
author: "Emma Klugman"
date: "8/2/2020"
output:
  word_document: default
  md_document:
    variant: markdown_github
  html_document:
    df_print: paged
  pdf_document: default
---


In this tutorial we show how schools, districts, and states can create and score a test comprised of publicly available questions. As our example, we use released items from the 2013 administration of NAEP 4th Grade Math.

# Ingredients:
1) Released test questions (items) to create the test, like these:

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/NationsReportCard/nqt/Search", out.width = '70%', fig.align="center"}
knitr::opts_chunk$set(fig.align="center", cache = TRUE)
knitr::include_graphics("images/1_released_items.png")
```

2) Released item IRT parameter estimates, like those found here: 
(Note that for NAEP, you will need to combine IRT parameters from each of the subscales to create an overall item pool)

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_irt_math.aspx", out.width = '70%', fig.align="center"}
knitr::include_graphics("images/2_item_parameters.png")
```

3) Common item numbers/identifiers that relate each question (item) to its parameters:
(To get the IDs for questions from NAEP, using the NAEP Questions Tool to select multiple choice questions from your chosen year, click OK, then select all items, then "Export to Excel") 

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/NationsReportCard/nqt/Search#", out.width = '70%', fig.align="center"}
knitr::include_graphics("images/3_questions_with_IDs.png")
```

4) Student scores. For our tutorial, these can either be sum scores, or correct/incorrect responses for each of the test questions. 
(For NAEP items, you should consider exploring "Download...Test Kit" for paper administration or "Publish Test Online" for online administration once you've chosen your items in the NAEP Questions Tool.)

5) An equation that converts theta scores to reporting scale scores (and possibly also achievement levels), like this:

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/nationsreportcard/tdw/analysis/trans_constants.aspx", out.width = '70%', fig.align="center"}
knitr::include_graphics("images/5_theta_to_scale.png")
```

For NAEP, cut scores separating achievement level can be found here: https://nces.ed.gov/nationsreportcard/mathematics/achieve.asp#grade4

# Contents:
### Estimated Student Scale Scores from Sum Scores:
* Import 3PL IRT item parameters
* Import theta to scale score equation or table
* Import (or simulate) student response data
* Estimate student thetas using sum scores
* Convert estimated thetas to scale scores
* Produce table converting sum scores to scale scores, using Test Characteristic Curve information
* Export student ability data, including estimated thetas, scale scores, and achievement levels

### Appendix 1: Simulating Imprecision for Sum Score Theta Estimates
* Invert the Test Characteristic Curve to Produce Estimates of Standard Errors
* Produce table converting sum scores to scale scores, with empirical standard errors, using simulations

### Appendix 2: Scale Score Estimation Using Full-Pattern Scoring
* Estimate student ability using full-pattern scoring, (includes standard errors)
* Export student ability data, including estimated thetas, standard errors, scale scores, and achievement levels

### Appendix 3: Diagnostics
* Report Classical Test Theory statistics
* Plot Item Characteristic Curves and Test Characteristic Curve
* Plot Item Information Functions and Test Information Function

### Appendix 4: Replicating NAEP's theta to scale score equations
* Import table of theta to scale score equations (NAEP reports these separately by subscale)
* Provide function to match NAEP's theta to scale score conversion, weighting transformation parameters appropriately

### Appendix 5: Creating an item map, using released item parameters
* Uses 3PL model to generate response probabilities for items
* Creates table of item descriptions connected to scale scores for a chosen response probability

```{r, include = FALSE, message = FALSE}
# Install/update needed packages
#install.packages("dplyr")
#install.packages("irtoys")      # May require some dependencies

library(dplyr)
library(irtoys)
```

# Estimated Student Scale Scores from Sum Scores:

### Importing item parameters and scale scores

```{r}
# Import selected items
selected_items <- read.csv("Selected_Items.csv", skip = 1)

# Pull out vector of NAEP IDs for selected items
item_IDs <- selected_items$NAEP.ID

# Import item parameter data
item_parameters_raw <- read.csv("IRT_Parameters.csv")                            

# Subset the item parameters, keeping only those whose IDs are in item_IDs
item_parameters_selected <- subset(item_parameters_raw, item_parameters_raw$NAEP.ID %in% item_IDs)

# Drop some unnecessary rows
item_parameters <- item_parameters_selected %>% dplyr::select(NAEP.ID, aj, bj, cj, Subscale)

# Turn this into the matrix form that irtoys expects
my_ip <- as.matrix(item_parameters  %>% dplyr::select(aj, bj, cj))               

# NAEP Technical Documentation shows that a normalizing constant D of 1.7 is used in their 3PL equations:
# See here: https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_models_3pl.aspx
# The R package irtoys uses a normalizing constant of 1, so we adjust all NAEP parameters accordingly
D <- 1.7          # Change to D = 1 if none appears in your technical report
# Divide "a" parameters by D to "undo" the normalizing constant and fit what the package irtoys expects
my_ip[,1] <- my_ip[,1]/D

# View first five rows
my_ip[1:5, ]

# Import student responses (if using real data)


# Takes theta(s), returns scale score(s) (for most tests, this looks like "scale_score = scale_multiplier*theta + scale_contant")
theta_to_scale_score <- function(theta)
{
  # NAEP uses a more complex weighted formula; we replicate that in Appendix 4
  # We simplify here by using the 2013 Grade 4 NAEP Math parameters for Number Properties and Operations, found here:
  # https://nces.ed.gov/nationsreportcard/tdw/analysis/2013/trans_constants_math2013.aspx
  scale_multiplier = 31.90
  scale_constant = 240.98
  return(scale_score = scale_multiplier*theta + scale_constant)
}


# Takes theta(s), returns achievement levels
scale_score_to_achievement_level <- function(scale_scores, cutoffs = c(214, 249, 282))
{
  # Note that these cutscores can vary by grade, subject, and year
  # Default cutoffs taken from here for 2013 grade 4 Math: 
  # https://nces.ed.gov/nationsreportcard/mathematics/achieve.asp#grade4
  achievement_levels <- rep("Below Basic", length(scale_scores)) 
  achievement_levels[scale_scores >= cutoffs[1]] <- "Basic"
  achievement_levels[scale_scores >= cutoffs[2]] <- "Proficient"
  achievement_levels[scale_scores >= cutoffs[3]] <- "Advanced"
  return(achievement_levels)
}
```

### Scale Score Estimation Using Sum Scoring
"Invert" the TCC to get a map from sum scores to thetas:

```{r}
# Plot Test Characteristic Curve (which underlies all of what follows)
# Theta on x axis, predicted sum score on y axis
plot(trf(my_ip))

# Create a range of possible thetas
thetas = seq(from = -5, to = 5, length.out = 100001)
# Calculate the estimated sum score for each of these thetas 
all_sum_scores <- trf(my_ip, x = thetas)

# Simplifies the above possible sum scores (decimals) into possible whole number scores
possible_sum_scores <- seq(from = ceiling(all_sum_scores$f[1]), to = floor(all_sum_scores$f[100000]), by = 1)

# For each possible whole number score, find the associated theta that's the best match
matching_theta_scores <- rep(NA, times = length(possible_sum_scores))
for(i in 1:length(possible_sum_scores)) {matching_theta_scores[i] <- all_sum_scores$x[which.min(abs(all_sum_scores$f - possible_sum_scores[i]))]}

# Produce a table with sum scores, thetas, scale scores, and achievement levels
matching_theta_scores_df <- as.data.frame(cbind(possible_sum_scores, matching_theta_scores))
colnames(matching_theta_scores_df) <- c("SumScore", "Theta")

# Convert the thetas to scale scores and then achievement levels, using our custom functions above
matching_theta_scores_df$ScaleScore <- theta_to_scale_score(matching_theta_scores_df$Theta)
matching_theta_scores_df$AchievementLevel <- scale_score_to_achievement_level(matching_theta_scores_df$ScaleScore)

# Round the Scale Scores before reporting
matching_theta_scores_df$ScaleScore <- round(matching_theta_scores_df$ScaleScore)

# View table
matching_theta_scores_df

# Export table
write.csv(matching_theta_scores_df, file = "Estimated_Scale_Scores_From_Sum_Scores.csv", row.names = F)
```

# Appendix 1: Simulating Imprecision for Sum Score Theta Estimates

How do we convey a sense of uncertainty with these sorts of score estimates?

Because students with "fixed" thetas can retake the test and score slightly differently each time, we simulate this below to create an empirical range of possible scale scores for each student sum score.

```{r warning=FALSE, message = FALSE}
# Create a sequence of possible thetas, equally spaced
sim_thetas <- seq(from = -5, to = 5, length.out = 1001)

# Replicate each of the possible thetas 100 times, as if each of these students took the test 100 times
sim_thetas_n <- rep(sim_thetas, each = 100)

# Simulate the sum scores that these students received, using the item parameters for this test (assume these item parameters are estimated perfectly)
sim_sum_scores <- rowSums(sim(my_ip, sim_thetas_n))

# Combine the thetas and sum scores into a data frame
sim1_long <- as.data.frame(cbind(sim_thetas_n, sim_sum_scores))

# Rename column to the name expected by theta_to_scale_score conversion
colnames(sim1_long) <- c("Theta", "sim_sum_scores")

# Add column for scale scores (from thetas)
sim1_long$sim_scale_scores <- theta_to_scale_score(sim1_long$Theta)

# Group the above data frame by sum scores, and summarize the different "true" thetas that could have produced each sum score
sim1_thetas <- sim1_long %>% 
  group_by(sim_sum_scores) %>% 
  summarise(tibble('2.5' = round(quantile(sim_scale_scores, 0.025), 0),
                   '10' = round(quantile(sim_scale_scores, 0.1), 0),
                   mean = round(mean(sim_scale_scores), 0), 
                   median = round(median(sim_scale_scores), 0), 
                   se = round(sd(sim_scale_scores), 1),
                   '90' = round(quantile(sim_scale_scores, 0.9), 0),
                   '97.5' = round(quantile(sim_scale_scores, 0.975), 0), 
                   max = round(max(sim_scale_scores), 0),
                   nsims = n()))
sim1_thetas

# Repeat the above, this time summarizing the different "true" scale scores that could have produced each sum score
sim1_scale_scores <- sim1_long %>% 
  group_by(sim_sum_scores) %>% 
  summarise(tibble('2.5' = round(quantile(sim_scale_scores, 0.025), 0),
                   '10' = round(quantile(sim_scale_scores, 0.1), 0),
                   mean = round(mean(sim_scale_scores), 0), 
                   median = round(median(sim_scale_scores), 0), 
                   se = round(sd(sim_scale_scores), 1),
                   '90' = round(quantile(sim_scale_scores, 0.9), 0),
                   '97.5' = round(quantile(sim_scale_scores, 0.975), 0), 
                   max = round(max(sim_scale_scores), 0),
                   nsims = n()))
sim1_scale_scores

# Can also create a histogram for the possible thetas that produced each sum score, for example:
hist(sim1_long$Theta[sim1_long$sim_sum_scores == 20], freq = F)
```

# Appendix 2: Scale Score Estimation Using Full-Pattern Scoring

Full-pattern scoring (using a full table recording how each student performed on each question) can improve ability estimation.

```{r}
# Use parameter estimates to simulate answers for 100 students
# (Import student data here if using real answers)
set.seed(88)
sim_thetas <- rnorm(1000)
sim_responses <- sim(my_ip, sim_thetas)

# Put the parameter estimates and standard errors into the list structure that irtoys functions expect
# Note: ability estimation function does not need standard errors or variance-covariance matrix to run
parameter_list <- list(est = my_ip, se = NA, vcm = NA)

# Estimate student thetas, based on full-pattern scoring, using MLE (several other methods exist for this "ability" function)
mod_MLE<- ability(resp = sim_responses, ip = parameter_list, method = "MLE")

# Look at the first five rows
mod_MLE[1:5, ]

# Convert these estimated thetas to scale scores and achievement levels
ability_df <- as.data.frame(mod_MLE)
colnames(ability_df) <- c("Theta", "se(Theta)", "QuestionsAnswered")
ability_df$ScaleScore <- theta_to_scale_score(ability_df$Theta)
ability_df$AchievementLevel <- scale_score_to_achievement_level(ability_df$ScaleScore)

# Round the scale scores to 0 dp before reporting
ability_df$ScaleScore <- round(ability_df$ScaleScore, 0)

# View table:
ability_df

# Export the data 
write.csv(ability_df, file = "Estimated_Scores_From_Full_Pattern_Scores.csv", row.names = F)
```


# Appendix 3: Diagnostics

## Diagnostics and other test plots:
```{r}
# Simulate data
set.seed(88)
sim_thetas <- rnorm(1000)
sim_responses <- sim(my_ip, sim_thetas)

# Classical Test Theory EDA metrics:
# Note: Because the simulated data is pre-graded, we're saying the "answer key" is 1, 1, 1, 1, 1....
ctt <- tia(sim_responses, key = rep(1, 31))

# Show Cronbach's alpha for this "test":
ctt$testlevel$alpha

# Show CTT item-level statistics for first five items
ctt$itemlevel[1:5, ]

# Estimate student thetas, based on full-pattern scoring, using MLE (several other methods exist for this "ability" function)
mod_MLE<- ability(resp = sim_responses, ip = parameter_list, method = "MLE")

# How does the MLE ability estimation perform compared to the "true thetas" from our simulation?
cor(mod_MLE[,1], sim_thetas)
plot(sim_thetas, mod_MLE[,1])

# How do other ability estimation methods perform?
mod_WLE <- ability(resp = sim_responses, ip = parameter_list, method = "WLE")
cor(mod_WLE[,1], sim_thetas)
plot(sim_thetas, mod_WLE[,1])

# Item Characteristic Curves for first five items (this package calls them "item response functions")
plot(irf(my_ip, items = 1:5), co = NA, label = T)

# ICCs for all items:
plot(irf(my_ip), co = NA, label = T)

# Test Characteristic Curve (this package calls this a "test response function")
plot(trf(my_ip))

# Overlaid item information curves
plot(iif(my_ip))

# Test information function
plot(tif(my_ip))

# Cool plot of observed sum scores and predicted sum scores against estimated ability, with +/- 1se bands
scp(sim_responses, my_ip)

# "Empirical response function" for a selected item: observed sum scores vs. percent correct on this question
item_1_erf <- erf(sim_responses, 1)
```


# Appendix 4: Replicating NAEP's theta to scale score equations

NAEP's theta to scale score transforming equations are separated by sub-scale, like this:

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/nationsreportcard/tdw/analysis/2013/trans_constants_math2013.aspx", out.width = '70%', fig.align="center"}
knitr::include_graphics("images/5b_theta_to_scale_table.png")
```

We provide an alternative theta_to_scale_score function below, which can weight these subscale parameters either by default proportions or by their proportions in your selected test items, to most accurately mimic NAEP scoring.

```{r}
# Import table of transformation constants
scale_scores_raw <- read.csv("Scale_Score_Equations.csv")  
  
# Takes theta(s), returns scale score(s)
theta_to_scale_score <- function(theta, weights = c(0.4, 0.2, 0.15, 0.1, 0.15))
{
  # Complex version: weighted average of each of the five subscale equations
  # Default weights taken from here for 2013 grade 4 Math: 
  # https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_determination_composite.aspx
  # Alternatively, user can provide weights customized to the actual distribution of questions used
  A = weighted.mean(x = scale_scores_raw$A, w = weights)
  B = weighted.mean(x = scale_scores_raw$B, w = weights)
  return(scale_score = A*theta + B)
}

```


# Appendix 5: Creating an item map, using released item parameters

An item map takes test questions and pegs them to specific scale scores, such that users can visualize which items a student of a given scale score is likely have already mastered, and which items they are less likely to answer correctly yet. 

Example item map: https://www.nationsreportcard.gov/itemmaps/?subj=MAT&grade=4&year=2013

NAEP says this about their item maps:

"NOTE: CR denotes a constructed-response question. SR denotes a selected-response question. MC denotes a multiple-choice question. Apparent differences between estimates may not be statistically significant. Results are not shown for data points where the sample sizes are insufficient to permit reliable estimates or where data are not available. The position of a question on the scale represents the scale score attained by students who had a 65 percent probability of obtaining credit at a specific level of CR questions or polytomously scored SR questions, a 74 percent probability of correctly answering a four-option multiple-choice question, or a 72 percent probability of correctly answering a five-option multiple-choice question in certain subjects."

In this section, we provide simple code to create item maps for any set of items for which item difficulties are known


```{r}
# Combine the table containing question descriptors (selected_items) with the table of item parameters (item_parameters)
combined_table <- inner_join(selected_items, item_parameters, by = "NAEP.ID", copy = FALSE)

# This function takes item parameter(s) and a response probability and returns theta(s)
ip_and_RP_to_theta <- function(data, RP = 0.73)
{
  # RP = response probability ("a student with scale score x should have a RP% chance of answering this item correctly")
  # RP defaults to 73% here, as NAEP uses RP = 74 for MC items with four choices, and RP = 72 for those with five options. 
  # We split the difference here
  theta = data$bj - 1/{D*data$aj}*log((1 - RP)/(RP - data$cj))
  return(theta)
}

combined_table$map_theta <- ip_and_RP_to_theta(data = combined_table)

# Convert to scale score, round to 0dp
combined_table$Scale_Score <- round(theta_to_scale_score(theta = combined_table$map_theta), 0)

# Sort by map_theta
combined_table <- combined_table[order(combined_table$map_theta, decreasing = TRUE), ]

# Export out, in format similar to NAEP
write.csv(combined_table[ , colnames(combined_table) %in% c("Subscale", "Scale_Score", "Description")], 
          file = "Generated_Item_Map.csv", row.names = F)
```