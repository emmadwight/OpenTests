---
title: 'Open Tests: A recipe for standards-based score reports using publicly released state test questions.'
author: "Emma Klugman and Andrew Ho, Harvard Graduate School of Education"
date: "8/6/2020"
output:
  html_document:
    df_print: paged
  md_document:
    variant: markdown_github
  pdf_document: default
  word_document: default
---

Download the latest version of this code here: https://github.com/emmaklugman/OpenTests

In this tutorial we show how school, district, and state assessment coordinators can create, administer, and score a test comprised of publicly available questions. Please read our accompanying article for additional details, *Can released state test items support diagnostic purposes in a COVID-19-disrupted year?*

Our article discusses many important cautions and caveats for use in the COVID-19 era, three of which we list here: 

1. Among tests, the tests we describe here should be lower in priority than tests of physical, mental, and social-emotional health, and lower in priority than classroom and district tests that may already be in place; 

2. We advise the use of these tests for aggregate (group-level) monitoring with no direct stakes on schools, educators, or students; and 

3. Score interpretations should be tempered by departures from typical administration conditions, where tests follow aligned instructional sequences and are administered in school settings.

In this illustration, we use released questions (items) from the 2013 administration of NAEP 4th Grade Math. Users can gather corresponding ingredients from state websites or request them from state testing programs. This recipe assumes expertise at the level of a first-year survey course in educational measurement.

# Ingredients:
We list the 5 essential ingredients for score reports here:

1. Test items

2. Item parameter estimates 

3. A list or key enabling association of items and their corresponding estimates

4. Linking functions from underlying theta scales to scale scores

5. Achievement level cut scores

<!-- 1) Released test questions (items) to create the test, like these: -->

<!-- ```{r, echo=FALSE, fig.cap="https://nces.ed.gov/NationsReportCard/nqt/Search", out.width = '70%', fig.align="center"} -->
<!-- knitr::opts_chunk$set(fig.align="center", cache = TRUE) -->
<!-- knitr::include_graphics("images/1_released_items.png") -->
<!-- ``` -->

<!-- 2) Released item IRT parameter estimates, like those found here:  -->
<!-- (Note that for NAEP, you will need to combine IRT parameters from each of the subscales to create an overall item pool) -->

<!-- ```{r, echo=FALSE, fig.cap="https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_irt_math.aspx", out.width = '70%', fig.align="center"} -->
<!-- knitr::include_graphics("images/2_item_parameters.png") -->
<!-- ``` -->

<!-- 3) Common item numbers/identifiers that relate each question (item) to its parameters: -->
<!-- (To get the IDs for questions from NAEP, using the NAEP Questions Tool to select multiple choice questions from your chosen year, click OK, then select all items, then "Export to Excel")  -->

<!-- ```{r, echo=FALSE, fig.cap="https://nces.ed.gov/NationsReportCard/nqt/Search#", out.width = '70%', fig.align="center"} -->
<!-- knitr::include_graphics("images/3_questions_with_IDs.png") -->
<!-- ``` -->

<!-- 4) Student scores. For our tutorial, these can either be sum scores, or correct/incorrect responses for each of the test questions.  -->
<!-- (For NAEP items, you should consider exploring "Download...Test Kit" for paper administration or "Publish Test Online" for online administration once you've chosen your items in the NAEP Questions Tool.) -->

<!-- 5) An equation that converts theta scores to reporting scale scores (and possibly also achievement levels), like this: -->

<!-- ```{r, echo=FALSE, fig.cap="https://nces.ed.gov/nationsreportcard/tdw/analysis/trans_constants.aspx", out.width = '70%', fig.align="center"} -->
<!-- knitr::include_graphics("images/5_theta_to_scale.png") -->
<!-- ``` -->

<!-- For NAEP, cut scores separating achievement level can be found here: https://nces.ed.gov/nationsreportcard/mathematics/achieve.asp#grade4 -->

# Contents:
#### Recipe: Estimated Student Scale Scores from Sum Scores:
* Import 3PL IRT item parameters
* Import theta to scale score equation or table
* Import (or simulate) student response data
* Estimate student thetas using sum scores
* Convert estimated thetas to scale scores
* Produce table converting sum scores to scale scores, using Test Characteristic Curve information
* Uses 3PL model to generate response probabilities for each item, for item mapping 
* Create table of item descriptions connected to scale scores for a chosen response probability
* Export student ability data, including estimated thetas, scale scores, achievement levels, and corresponding exemplar items

#### Appendix 1: Imprecision in Scale Score Estimates
* (Under construction)
<!-- ### Appendix 1: Simulating Imprecision for Sum Score Theta Estimates -->
<!-- * Invert the Test Characteristic Curve to Produce Estimates of Standard Errors -->
<!-- * Produce table converting sum scores to scale scores, with empirical standard errors, using simulations -->

#### Appendix 2: Scale Score Estimation Using Full-Pattern Scoring
* Estimate student ability using full-pattern scoring, (includes standard errors)
* Export student ability data, including estimated thetas, standard errors, scale scores, and achievement levels

#### Appendix 3: Replicating NAEP's theta to scale score equations
* Import table of theta to scale score equations (NAEP reports these separately by subscale)
* Provide function to match NAEP's theta to scale score conversion, weighting transformation parameters appropriately

```{r, include = FALSE, message = FALSE}
# Install/update needed packages
#install.packages("dplyr")
#install.packages("irtoys")      # May require some dependencies
#install.packages("fuzzyjoin")

library(dplyr)
library(irtoys)
library(fuzzyjoin)
```

# Recipe: Estimated Student Scale Scores from Sum Scores:

#### Importing item parameters and scale scores

```{r}
# Import selected items
# NAEP example from: https://nces.ed.gov/NationsReportCard/nqt/Search
selected_items <- read.csv("Selected_Items.csv", skip = 1)

# Import item parameter data
# NAEP example from: https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_irt_math.aspx 
item_parameters_raw <- read.csv("IRT_Parameters.csv")                            
# Join the two tables, using "NAEP.ID" as the key
merged_data_raw <- inner_join(x = selected_items, y = item_parameters_raw, by = "NAEP.ID")

# Drop some unnecessary rows
merged_data <- dplyr::select(merged_data_raw, -c(Question.ID, Block.ID, Type, Seq., Online, dj1, dj2, dj3, dj4))

# Turn the item parameters into the matrix form that irtoys expects
ip <- as.matrix(dplyr::select(merged_data, aj, bj, cj))               
# NAEP Technical Documentation shows that a normalizing constant D of 1.7 is used in their 3PL equations: https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_models_3pl.aspx
D <- 1.7      # Change to D = 1 if no D appears in your technical manual    

# The R package irtoys uses a normalizing constant of 1, so we "undo" the normalizing constant first
ip[,1] <- ip[,1]/D

# Takes theta(s), returns scale score(s) (for most tests, this looks like "scale_score = scale_multiplier*theta + scale_contant")
theta_to_scale_score <- function(theta, M = 31.90, K = 240.98)
{
  # NAEP uses a weighted formula; we replicate that in Appendix 4
  # Here, our defaults for M and K are from the 2013 Grade 4 NAEP Math parameters for Number Properties and Operations:
  # https://nces.ed.gov/nationsreportcard/tdw/analysis/2013/trans_constants_math2013.aspx
  return(scale_score = M*theta + K)
}

# Takes theta(s), returns achievement levels
scale_score_to_achievement_level <- function(scale_scores, cutoffs = c(214, 249, 282))
{
  # Note that these cutscores usually vary by grade, subject, and year
  # Our default cutoffs taken from here for 2013 grade 4 Math: 
  # https://nces.ed.gov/nationsreportcard/mathematics/achieve.asp#grade4
  achievement_levels <- rep("Below Basic", length(scale_scores)) 
  achievement_levels[scale_scores >= cutoffs[1]] <- "Basic"
  achievement_levels[scale_scores >= cutoffs[2]] <- "Proficient"
  achievement_levels[scale_scores >= cutoffs[3]] <- "Advanced"
  return(achievement_levels)
}
```

#### Scale Score Estimation Using Sum Scoring
"Invert" the TCC to get a map from sum scores to thetas:

```{r}
# Plot Test Characteristic Curve (which underlies all of what follows)
# Theta on x axis, predicted sum score on y axis
plot(trf(ip))

# Create a range of possible thetas
thetas = seq(from = -10, to = 10, length.out = 200001)
# Calculate the estimated sum score for each of these thetas 
all_sum_scores <- trf(ip, x = thetas)

# Simplifies the above possible sum scores (decimals) into possible whole number scores
possible_sum_scores <- seq(from = ceiling(all_sum_scores$f[1]), to = floor(all_sum_scores$f[200001]), by = 1)

# For each possible whole number score, find the associated theta that's the best match
matching_theta_scores <- rep(NA, times = length(possible_sum_scores))
for(i in 1:length(possible_sum_scores)) {matching_theta_scores[i] <- all_sum_scores$x[which.min(abs(all_sum_scores$f - possible_sum_scores[i]))]}

# Produce a table with sum scores, thetas, scale scores, and achievement levels
matching_theta_scores_df <- as.data.frame(cbind(possible_sum_scores, matching_theta_scores))
colnames(matching_theta_scores_df) <- c("SumScore", "Theta")

# Convert the thetas to scale scores and then achievement levels, using our custom functions above
matching_theta_scores_df$ScaleScore <- theta_to_scale_score(matching_theta_scores_df$Theta)
matching_theta_scores_df$AchievementLevel <- scale_score_to_achievement_level(matching_theta_scores_df$ScaleScore)

# Round the Scale Scores before reporting
matching_theta_scores_df$ScaleScore <- round(matching_theta_scores_df$ScaleScore)

# View table
matching_theta_scores_df

# Export table
write.csv(matching_theta_scores_df, file = "Estimated_Scale_Scores_From_Sum_Scores.csv", row.names = F)
```

### Creating an item map, using released item parameters

An item map takes test questions and pegs them to specific scale scores, such that users can visualize which items a student of a given scale score is likely have already mastered, and which items they are less likely to answer correctly yet. 

Example item map: https://www.nationsreportcard.gov/itemmaps/?subj=MAT&grade=4&year=2013

In this section, we provide simple code to create item maps for any set of items for which item difficulties are known

```{r}
# This function takes item parameter(s) and a response probability and returns theta(s)
ip_and_RP_to_theta <- function(data, RP = 0.73)
{
  # RP = response probability ("a student with scale score x should have a RP% chance of answering this item correctly")
  # RP defaults to 73% here, as NAEP uses RP = 74 for MC items with four choices, and RP = 72 for those with five options. 
  # We split the difference here with an RP of 0.73
  theta = data$bj - (1/{D*data$aj}) * log((1 - RP)/(RP - data$cj))
  return(theta)
}

# Add a columns to item_parameters pegging them to a certain Theta with response probability RP 
merged_data$Theta <- ip_and_RP_to_theta(data = merged_data)

# Convert to scale score, round to 0dp
merged_data$ScaleScore <- round(theta_to_scale_score(theta = merged_data$Theta), 0)

# View first five rows
merged_data[1:5, ]

# Sort by Theta and export
write.csv(merged_data[order(merged_data$Theta, decreasing = TRUE), ], file = "Test_Item_Map.csv", row.names = F)
```

### Combining mapped items with sum score table

```{r}
# "Fuzzy join" the matching_theta_scores_df ScaleScore with the nearest item map scale score, within some max_dist tolerance
merge1 <- difference_left_join(x = matching_theta_scores_df,
                             y = merged_data,
                             by = "ScaleScore",
                             max_dist = 5,
                             distance_col = "Distance")

# Replace Distance = NA (for those with no item map question within max-dist) with 0
merge1$Distance[is.na(merge1$Distance)] <- 0

# For those Scale Scores with multiple item map matches within max_dist, pick the closest (smallest distance), if still multiples, pick first
merge2 <- merge1 %>% 
    group_by(SumScore) %>% 
    filter(Distance == min(Distance)) %>%
    group_by(SumScore) %>% 
    filter(row_number() == 1)

# Drop the distance column
merge2 <- subset(merge2, select = -c(Distance))

# Rename remaining columns for clarity
colnames(merge2) <- c("SumScore", "Theta", "ScaleScore", "AchievementLevel", "ClosestItemDescription", "ClosestItemSubscale", "ClosestItemScaleScore")

# View table
merge2[1:5, ]

write.csv(merge2, file = "MasterTable.csv", row.names = F)


# Try again, this time with ALL grade 4 math items:
# This is directly from NAEP maps, so already has scale scores
all_g4_items <- read.csv("Combined_Item_Maps.csv")
colnames(all_g4_items) <- c("ContentClassification", "ScaleScore", "Question", "Year")

# "Fuzzy join" the matching_theta_scores_df ScaleScore with the nearest item map scale score, within some max_dist tolerance
merge3 <- difference_left_join(x = matching_theta_scores_df,
                             y = all_g4_items,
                             by = "ScaleScore",
                             max_dist = 5,
                             distance_col = "Distance")

# Replace Distance = NA (for those with no item map question within max-dist) with 0
merge3$Distance[is.na(merge3$Distance)] <- 0

# For those Scale Scores with multiple item map matches within max_dist, pick the closest (smallest distance), if still multiples of equal distance, pick randomly
merge4 <- merge3 %>% 
    group_by(SumScore) %>% 
    filter(Distance == min(Distance)) %>%
    group_by(SumScore) %>% 
    #filter(row_number() == 1)
    sample_n(size = 1)
    
# Drop the distance column
merge4 <- subset(merge4, select = -c(Distance))

# Rename remaining columns for clarity
colnames(merge4)[which(names(merge4) == "ScaleScore.x")] <- "ScaleScore"
colnames(merge4)[which(names(merge4) == "ScaleScore.y")] <- "ClosestItemScaleScore"

# View table
merge4[1:5, ]

write.csv(merge4, file = "MasterTableBig.csv", row.names = F)
```

<!-- # Appendix 1: Simulating Imprecision for Sum Score Theta Estimates -->

<!-- How do we convey a sense of uncertainty with these sorts of score estimates? -->

<!-- Because students with "fixed" thetas can retake the test and score slightly differently each time, we simulate this below to create an empirical range of possible scale scores for each student sum score. -->

<!-- ```{r warning=FALSE, message = FALSE} -->
<!-- # Create a sequence of possible thetas, equally spaced -->
<!-- sim_thetas <- seq(from = -5, to = 5, length.out = 1001) -->

<!-- # Replicate each of the possible thetas 100 times, as if each of these students took the test 100 times -->
<!-- sim_thetas_n <- rep(sim_thetas, each = 100) -->

<!-- # Simulate the sum scores that these students received, using the item parameters for this test (assume these item parameters are estimated perfectly) -->
<!-- sim_sum_scores <- rowSums(sim(ip, sim_thetas_n)) -->

<!-- # Combine the thetas and sum scores into a data frame -->
<!-- sim1_long <- as.data.frame(cbind(sim_thetas_n, sim_sum_scores)) -->

<!-- # Rename column to the name expected by theta_to_scale_score conversion -->
<!-- colnames(sim1_long) <- c("Theta", "sim_sum_scores") -->

<!-- # Add column for scale scores (from thetas) -->
<!-- sim1_long$sim_scale_scores <- theta_to_scale_score(sim1_long$Theta) -->

<!-- # Group the above data frame by sum scores, and summarize the different "true" thetas that could have produced each sum score -->
<!-- sim1_thetas <- sim1_long %>%  -->
<!--   group_by(sim_sum_scores) %>%  -->
<!--   summarise(tibble('2.5' = round(quantile(sim_scale_scores, 0.025), 0), -->
<!--                    '10' = round(quantile(sim_scale_scores, 0.1), 0), -->
<!--                    mean = round(mean(sim_scale_scores), 0),  -->
<!--                    median = round(median(sim_scale_scores), 0),  -->
<!--                    se = round(sd(sim_scale_scores), 1), -->
<!--                    '90' = round(quantile(sim_scale_scores, 0.9), 0), -->
<!--                    '97.5' = round(quantile(sim_scale_scores, 0.975), 0),  -->
<!--                    max = round(max(sim_scale_scores), 0), -->
<!--                    nsims = n())) -->
<!-- sim1_thetas -->

<!-- # Repeat the above, this time summarizing the different "true" scale scores that could have produced each sum score -->
<!-- sim1_scale_scores <- sim1_long %>%  -->
<!--   group_by(sim_sum_scores) %>%  -->
<!--   summarise(tibble('2.5' = round(quantile(sim_scale_scores, 0.025), 0), -->
<!--                    '10' = round(quantile(sim_scale_scores, 0.1), 0), -->
<!--                    mean = round(mean(sim_scale_scores), 0),  -->
<!--                    median = round(median(sim_scale_scores), 0),  -->
<!--                    se = round(sd(sim_scale_scores), 1), -->
<!--                    '90' = round(quantile(sim_scale_scores, 0.9), 0), -->
<!--                    '97.5' = round(quantile(sim_scale_scores, 0.975), 0),  -->
<!--                    max = round(max(sim_scale_scores), 0), -->
<!--                    nsims = n())) -->
<!-- sim1_scale_scores -->

<!-- # Can also create a histogram for the possible thetas that produced each sum score, for example: -->
<!-- hist(sim1_long$Theta[sim1_long$sim_sum_scores == 20], freq = F) -->
<!-- ``` -->

# Appendix 2: Scale Score Estimation Using Full-Pattern Scoring

Full-pattern scoring (using a full table recording how each student performed on each question) can improve ability estimation.

```{r}
# Use parameter estimates to simulate answers for 100 students
# (Import student data here if using real answers)
set.seed(88)
sim_thetas <- rnorm(1000)
sim_responses <- sim(ip, sim_thetas)

# Put the parameter estimates and standard errors into the list structure that irtoys functions expect
# Note: ability estimation function does not need standard errors or variance-covariance matrix to run
parameter_list <- list(est = ip, se = NA, vcm = NA)

# Estimate student thetas, based on full-pattern scoring, using MLE (several other methods exist for this "ability" function)
mod_MLE<- ability(resp = sim_responses, ip = parameter_list, method = "MLE")

# Look at the first five rows
mod_MLE[1:5, ]

# Convert these estimated thetas to scale scores and achievement levels
ability_df <- as.data.frame(mod_MLE)
colnames(ability_df) <- c("Theta", "se(Theta)", "QuestionsAnswered")
ability_df$ScaleScore <- theta_to_scale_score(ability_df$Theta)
ability_df$AchievementLevel <- scale_score_to_achievement_level(ability_df$ScaleScore)

# Round the scale scores to 0 dp before reporting
ability_df$ScaleScore <- round(ability_df$ScaleScore, 0)

# View first five rows of table:
ability_df[1:5, ]

# Export the data 
# write.csv(ability_df, file = "Estimated_Scores_From_Full_Pattern_Scores.csv", row.names = F)
```

# Appendix 3: Replicating NAEP's theta to scale score equations

NAEP's theta to scale score transforming equations are separated by sub-scale, like this:

```{r, echo=FALSE, fig.cap="https://nces.ed.gov/nationsreportcard/tdw/analysis/2013/trans_constants_math2013.aspx", out.width = '70%', fig.align="center"}
knitr::include_graphics("images/5b_theta_to_scale_table.png")
```

We provide an alternative theta_to_scale_score function below, which can weight these subscale parameters either by default proportions or by their proportions in your selected test items, to most accurately mimic NAEP scoring.

```{r}
# Import table of transformation constants
scale_scores_raw <- read.csv("Scale_Score_Equations.csv")  
  
# Takes theta(s), returns scale score(s)
theta_to_scale_score <- function(theta, weights = c(0.4, 0.2, 0.15, 0.1, 0.15))
{
  # Complex version: weighted average of each of the five subscale equations
  # Default weights taken from here for 2013 grade 4 Math: 
  # https://nces.ed.gov/nationsreportcard/tdw/analysis/scaling_determination_composite.aspx
  # Alternatively, user can provide weights customized to the actual distribution of questions used
  A = weighted.mean(x = scale_scores_raw$A, w = weights)
  B = weighted.mean(x = scale_scores_raw$B, w = weights)
  return(scale_score = A*theta + B)
}

```